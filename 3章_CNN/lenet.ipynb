{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>第三章</h1>\n",
    "一番最初のCNN、LeNetの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten,Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>LeNetを作成する関数作成</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet(input_shape,num_classes):\n",
    "    model=Sequential()\n",
    "    #画像の特徴を抽出するconvolution層（フィルターによる畳み込み計算）とmax pooling層のレイヤー定義\n",
    "    #5*5のフィルター20個で畳み込みを行い、Reluにより次の層に伝播させる。padding=\"same\"より出力される特徴マップのサイズは入力サイズと同じ28*28になる。\n",
    "    #フィルター数が20のため、28*28*20の特徴マップが出力される。\n",
    "    model.add(Conv2D(20,kernel_size=5,padding=\"same\",\n",
    "                    input_shape=input_shape,activation=\"relu\"))\n",
    "    #2*2毎にプーリングするため、出力される特徴マップは、14*14*20となる。\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    #Conv2Dで再度、畳み込みを行う。層が深くなるにつれて、フィルター数を増やすことはCNNの一般的なテクニックである。出力される特徴マップのサイズは14*14*50\n",
    "    model.add(Conv2D(50,kernel_size=5,padding=\"same\",\n",
    "                    activation=\"relu\"))\n",
    "    #再度プーリングする。出力されるサイズは7*7*50\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    #全結合層により分類を行う\n",
    "    #マトリクスのデータである特徴マップをベクトルに変換し、全結合層に入力できるようにする。\n",
    "    model.add(Flatten())\n",
    "    #Denseにより特徴マップをならしたベクトルから、サイズ500のベクトルを抽出する。（次元圧縮）\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    #前層のDenseから、分類するクラス分の10ベクトルに抽出。\n",
    "    model.add(Dense(num_classes))\n",
    "    #ソフトマックス関数で正規化\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>学習データとしてmnistを用意する</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset():\n",
    "    def __init__(self):\n",
    "        self.image_shape=(28,28,1)#画像サイズ（グレースケールのためチャンネルは1）\n",
    "        self.num_classes=10\n",
    "    \n",
    "    def get_batch(self):\n",
    "        (x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "        x_train,x_test=[self.preprocess(d) for d in [x_train,x_test]]\n",
    "        y_train,y_test=[self.preprocess(d,label_data=True) for d in [y_train,y_test]]\n",
    "        return x_train,y_train,x_test,y_test\n",
    "    \n",
    "    def preprocess(self,data,label_data=False):\n",
    "        if label_data:\n",
    "            #正解ラベルをバイナリデータに変換する\n",
    "            data=keras.utils.to_categorical(data,self.num_classes)\n",
    "        else:\n",
    "            data=data.astype(\"float32\")\n",
    "            data/=255#dataを0~1で正規化\n",
    "            shape=(data.shape[0],)+self.image_shape\n",
    "            data=data.reshape(shape)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>モデルを学習させる関数作成</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "\n",
    "    def __init__(self, model, loss, optimizer):\n",
    "        self._target = model\n",
    "        self._target.compile(\n",
    "            loss=loss, optimizer=optimizer, metrics=[\"accuracy\"]\n",
    "            )\n",
    "        self.verbose = 1\n",
    "        logdir = \"logdir_lenet\"\n",
    "        self.log_dir = os.path.join(os.path.dirname(__file__), logdir)\n",
    "\n",
    "    def train(self, x_train, y_train, batch_size, epochs, validation_split):\n",
    "        if os.path.exists(self.log_dir):\n",
    "            import shutil\n",
    "            shutil.rmtree(self.log_dir)  # remove previous execution\n",
    "        os.mkdir(self.log_dir)\n",
    "\n",
    "        self._target.fit(\n",
    "            x_train, y_train,\n",
    "            batch_size=batch_size, epochs=epochs,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[TensorBoard(log_dir=self.log_dir)],\n",
    "            verbose=self.verbose\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>定義した関数を呼び出すメイン処理</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-98ab8fda011f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m trainer=Trainer(model,loss=\"categorical_crossentropy\",\n\u001b[0;32m----> 7\u001b[0;31m                optimizer=Adam())\n\u001b[0m\u001b[1;32m      8\u001b[0m trainer.train(x_train,y_train,batch_size128,epoch=12,\n\u001b[1;32m      9\u001b[0m              validation_split=0.2)\n",
      "\u001b[0;32m<ipython-input-21-6182d098ad78>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, loss, optimizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mlogdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"logdir_lenet\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "dataset=MNISTDataset()\n",
    "#モデル作成\n",
    "model=lenet(dataset.image_shape,dataset.num_classes)\n",
    "#モデルの訓練\n",
    "x_train,y_train,x_test,y_test=dataset.get_batch()\n",
    "trainer=Trainer(model,loss=\"categorical_crossentropy\",\n",
    "               optimizer=Adam())\n",
    "trainer.train(x_train,y_train,batch_size128,epoch=12,\n",
    "             validation_split=0.2)\n",
    "\n",
    "#結果表示\n",
    "score=model.evaluate(x_test,y_test,verbose=0)\n",
    "print(\"test loss:\",score[0])\n",
    "print(\"test accuracy:\",score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
